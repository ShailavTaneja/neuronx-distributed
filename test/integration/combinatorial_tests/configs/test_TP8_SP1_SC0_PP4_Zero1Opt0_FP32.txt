NEURON_FLAGS=--model-type transformer --distribution-strategy=llm-training
GBS=256
MBS=1
SEQ_LEN=4096
TP_DEGREE=8
SEQUENCE_PARALLEL=1
SELECTIVE_CHECKPOINT=0
PIPELINE_PARALLEL=4
USE_ZERO_1=0
USE_MIX_PRECISION=1
